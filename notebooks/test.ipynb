{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVp2sAW2H7hA"
   },
   "source": [
    "# **Project 1 – NYC Taxi Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On one of the csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TE1o6RO_AD-l"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    unix_timestamp,\n",
    "    udf,\n",
    "    when,\n",
    "    sum as spark_sum,\n",
    "    count,\n",
    "    avg,\n",
    "    lag,\n",
    "    lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from shapely.geometry import shape, Point\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NYC Taxi Analysis\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"../data/trip_data/trip_data_1.csv\", header = True, inferSchema = True)\n",
    "\n",
    "df = df.na.drop(subset=[\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"])\n",
    "\n",
    "# df = df.limit(100) # Limit for testing purposes\n",
    "\n",
    "with open(\"../data/nyc-boroughs.geojson\") as f:\n",
    "    boroughs = json.load(f)\n",
    "\n",
    "# Convert borough boundaries to Shapely polygons\n",
    "borough_shapes = [(feature[\"properties\"][\"borough\"], shape(feature[\"geometry\"])) for feature in boroughs[\"features\"]]\n",
    "\n",
    "# Define UDF to find borough for a given coordinate\n",
    "def get_borough(lat, lon):\n",
    "    point = Point(lon, lat)\n",
    "    for borough, polygon in borough_shapes:\n",
    "        if polygon.contains(point):\n",
    "            return borough\n",
    "    return \"Unknown\"\n",
    "\n",
    "borough_udf = udf(get_borough, StringType())\n",
    "\n",
    "# Enrich taxi dataset with borough information\n",
    "df = df.withColumn(\"pickup_borough\", borough_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "       .withColumn(\"dropoff_borough\", borough_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "\n",
    "# Convert to unix timestamp\n",
    "df = df.withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\", \"dd-MM-yy HH:mm\")) \\\n",
    "      .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\", \"dd-MM-yy HH:mm\"))\n",
    "\n",
    "# Compute trip duration\n",
    "df = df.withColumn(\"trip_duration\", col(\"dropoff_ts\") - col(\"pickup_ts\"))\n",
    "\n",
    "# Remove outliers (negative duration or longer than 4 hours)\n",
    "df = df.filter((col(\"trip_duration\") > 0) & (col(\"trip_duration\") <= 14400))\n",
    "\n",
    "# Compute idle time for each taxi\n",
    "taxi_window = Window.partitionBy(\"medallion\").orderBy(\"pickup_ts\")\n",
    "df = df.withColumn(\"prev_dropoff\", lag(\"dropoff_ts\").over(taxi_window)) \\\n",
    "       .withColumn(\"idle_time\", when((col(\"pickup_ts\") - col(\"prev_dropoff\")) <= 14400, col(\"pickup_ts\") - col(\"prev_dropoff\")).otherwise(lit(0)))\n",
    "\n",
    "# Aggregate utilization per taxi\n",
    "taxi_utilization = df.groupBy(\"medallion\").agg((spark_sum(\"trip_duration\") / (spark_sum(\"trip_duration\") + spark_sum(\"idle_time\"))).alias(\"utilization\"))\n",
    "\n",
    "# Compute average time to find next fare per borough\n",
    "taxi_next_fare = df.groupBy(\"dropoff_borough\").agg(avg(\"idle_time\").alias(\"avg_time_to_next_fare\"))\n",
    "\n",
    "# Compute trip counts\n",
    "same_borough_trips = df.filter(col(\"pickup_borough\") == col(\"dropoff_borough\")).groupBy(\"pickup_borough\").agg(count(\"*\").alias(\"same_borough_trips\"))\n",
    "diff_borough_trips = df.filter(col(\"pickup_borough\") != col(\"dropoff_borough\")).groupBy(\"pickup_borough\", \"dropoff_borough\").agg(count(\"*\").alias(\"diff_borough_trips\"))\n",
    "\n",
    "# Show results – for testing purposes\n",
    "# taxi_utilization.show()\n",
    "# taxi_next_fare.show()\n",
    "# same_borough_trips.show()\n",
    "# diff_borough_trips.show()\n",
    "# total_zero_idle_time = df.filter(col(\"idle_time\") == 0).count()\n",
    "# print(total_zero_idle_time)\n",
    "\n",
    "taxi_next_fare_renamed = taxi_next_fare.withColumnRenamed(\"dropoff_borough\", \"dropoff_borough_nf\")\n",
    "\n",
    "df_final = df.select(\"medallion\", \"pickup_borough\", \"dropoff_borough\", \"trip_duration\", \"idle_time\") \\\n",
    "             .join(taxi_utilization, \"medallion\", \"left\") \\\n",
    "             .join(taxi_next_fare_renamed, df[\"dropoff_borough\"] == taxi_next_fare_renamed[\"dropoff_borough_nf\"], \"left\") \\\n",
    "             .join(same_borough_trips, \"pickup_borough\", \"left\") \\\n",
    "             .join(diff_borough_trips, [\"pickup_borough\", \"dropoff_borough\"], \"left\") \\\n",
    "             .drop(\"dropoff_borough_nf\")\n",
    "\n",
    "df_final.write.mode(\"append\").parquet(\"../data/results_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST!@!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Convert csvs to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSVtoParquet\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\") \\\n",
    "    .csv(\"../data/trip_data/*.csv\") \\\n",
    "\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/full_dataset_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TE1o6RO_AD-l"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    unix_timestamp,\n",
    "    udf,\n",
    "    when,\n",
    "    sum as spark_sum,\n",
    "    count,\n",
    "    avg,\n",
    "    lag,\n",
    "    lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from shapely.geometry import shape, Point\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .parquet(\"../data/full_dataset_parquet\")\n",
    "\n",
    "df = df.na.drop(subset=[\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"])\n",
    "\n",
    "#df = df.limit(100) # Limit for testing purposes\n",
    "\n",
    "with open(\"../data/nyc-boroughs.geojson\") as f:\n",
    "    boroughs = json.load(f)\n",
    "\n",
    "# Convert borough boundaries to Shapely polygons\n",
    "borough_shapes = [(feature[\"properties\"][\"borough\"], shape(feature[\"geometry\"])) for feature in boroughs[\"features\"]]\n",
    "\n",
    "# Define UDF to find borough for a given coordinate\n",
    "def get_borough(lat, lon):\n",
    "    point = Point(lon, lat)\n",
    "    for borough, polygon in borough_shapes:\n",
    "        if polygon.contains(point):\n",
    "            return borough\n",
    "    return \"Unknown\"\n",
    "\n",
    "borough_udf = udf(get_borough, StringType())\n",
    "\n",
    "# Enrich taxi dataset with borough information\n",
    "df = df.withColumn(\"pickup_borough\", borough_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "       .withColumn(\"dropoff_borough\", borough_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "\n",
    "# Convert to unix timestamp\n",
    "df = df.withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\", \"dd-MM-yy HH:mm\")) \\\n",
    "      .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\", \"dd-MM-yy HH:mm\"))\n",
    "\n",
    "# Compute trip duration\n",
    "df = df.withColumn(\"trip_duration\", col(\"dropoff_ts\") - col(\"pickup_ts\"))\n",
    "\n",
    "# Remove outliers (negative duration or longer than 4 hours)\n",
    "df = df.filter((col(\"trip_duration\") > 0) & (col(\"trip_duration\") <= 14400))\n",
    "\n",
    "# Compute idle time for each taxi\n",
    "taxi_window = Window.partitionBy(\"medallion\").orderBy(\"pickup_ts\")\n",
    "df = df.withColumn(\"prev_dropoff\", lag(\"dropoff_ts\").over(taxi_window)) \\\n",
    "       .withColumn(\"idle_time\", when((col(\"pickup_ts\") - col(\"prev_dropoff\")) <= 14400, col(\"pickup_ts\") - col(\"prev_dropoff\")).otherwise(lit(0)))\n",
    "\n",
    "# Aggregate utilization per taxi\n",
    "taxi_utilization = df.groupBy(\"medallion\").agg((spark_sum(\"trip_duration\") / (spark_sum(\"trip_duration\") + spark_sum(\"idle_time\"))).alias(\"utilization\"))\n",
    "\n",
    "# Compute average time to find next fare per borough\n",
    "taxi_next_fare = df.groupBy(\"dropoff_borough\").agg(avg(\"idle_time\").alias(\"avg_time_to_next_fare\"))\n",
    "\n",
    "# Compute trip counts\n",
    "same_borough_trips = df.filter(col(\"pickup_borough\") == col(\"dropoff_borough\")).groupBy(\"pickup_borough\").agg(count(\"*\").alias(\"same_borough_trips\"))\n",
    "diff_borough_trips = df.filter(col(\"pickup_borough\") != col(\"dropoff_borough\")).groupBy(\"pickup_borough\", \"dropoff_borough\").agg(count(\"*\").alias(\"diff_borough_trips\"))\n",
    "\n",
    "# Show results – for testing purposes\n",
    "# taxi_utilization.show()\n",
    "# taxi_next_fare.show()\n",
    "# same_borough_trips.show()\n",
    "# diff_borough_trips.show()\n",
    "# total_zero_idle_time = df.filter(col(\"idle_time\") == 0).count()\n",
    "# print(total_zero_idle_time)\n",
    "\n",
    "taxi_next_fare_renamed = taxi_next_fare.withColumnRenamed(\"dropoff_borough\", \"dropoff_borough_nf\")\n",
    "\n",
    "df_final = df.select(\"medallion\", \"pickup_borough\", \"dropoff_borough\", \"trip_duration\", \"idle_time\") \\\n",
    "             .join(taxi_utilization, \"medallion\", \"left\") \\\n",
    "             .join(taxi_next_fare_renamed, df[\"dropoff_borough\"] == taxi_next_fare_renamed[\"dropoff_borough_nf\"], \"left\") \\\n",
    "             .join(same_borough_trips, \"pickup_borough\", \"left\") \\\n",
    "             .join(diff_borough_trips, [\"pickup_borough\", \"dropoff_borough\"], \"left\") \\\n",
    "             .drop(\"dropoff_borough_nf\")\n",
    "\n",
    "df_final.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/full_results_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read csvs directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TE1o6RO_AD-l"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/10 17:52:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/10 17:52:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    unix_timestamp,\n",
    "    udf,\n",
    "    when,\n",
    "    sum as spark_sum,\n",
    "    count,\n",
    "    avg,\n",
    "    lag,\n",
    "    lit\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from shapely.geometry import shape, Point\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\") \\\n",
    "    .csv(\"../data/trip_data/*.csv\")\n",
    "\n",
    "df = df.na.drop(subset=[\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"])\n",
    "\n",
    "#df = df.limit(100) # Limit for testing purposes\n",
    "\n",
    "with open(\"../data/nyc-boroughs.geojson\") as f:\n",
    "    boroughs = json.load(f)\n",
    "\n",
    "# Convert borough boundaries to Shapely polygons\n",
    "borough_shapes = [(feature[\"properties\"][\"borough\"], shape(feature[\"geometry\"])) for feature in boroughs[\"features\"]]\n",
    "\n",
    "# Define UDF to find borough for a given coordinate\n",
    "def get_borough(lat, lon):\n",
    "    point = Point(lon, lat)\n",
    "    for borough, polygon in borough_shapes:\n",
    "        if polygon.contains(point):\n",
    "            return borough\n",
    "    return \"Unknown\"\n",
    "\n",
    "borough_udf = udf(get_borough, StringType())\n",
    "\n",
    "# Enrich taxi dataset with borough information\n",
    "df = df.withColumn(\"pickup_borough\", borough_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "       .withColumn(\"dropoff_borough\", borough_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "\n",
    "# Convert to unix timestamp\n",
    "df = df.withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\", \"dd-MM-yy HH:mm\")) \\\n",
    "      .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\", \"dd-MM-yy HH:mm\"))\n",
    "\n",
    "# Compute trip duration\n",
    "df = df.withColumn(\"trip_duration\", col(\"dropoff_ts\") - col(\"pickup_ts\"))\n",
    "\n",
    "# Remove outliers (negative duration or longer than 4 hours)\n",
    "df = df.filter((col(\"trip_duration\") > 0) & (col(\"trip_duration\") <= 14400))\n",
    "\n",
    "# Compute idle time for each taxi\n",
    "taxi_window = Window.partitionBy(\"medallion\").orderBy(\"pickup_ts\")\n",
    "df = df.withColumn(\"prev_dropoff\", lag(\"dropoff_ts\").over(taxi_window)) \\\n",
    "       .withColumn(\"idle_time\", when((col(\"pickup_ts\") - col(\"prev_dropoff\")) <= 14400, col(\"pickup_ts\") - col(\"prev_dropoff\")).otherwise(lit(0)))\n",
    "\n",
    "# Aggregate utilization per taxi\n",
    "taxi_utilization = df.groupBy(\"medallion\").agg((spark_sum(\"trip_duration\") / (spark_sum(\"trip_duration\") + spark_sum(\"idle_time\"))).alias(\"utilization\"))\n",
    "\n",
    "# Compute average time to find next fare per borough\n",
    "taxi_next_fare = df.groupBy(\"dropoff_borough\").agg(avg(\"idle_time\").alias(\"avg_time_to_next_fare\"))\n",
    "\n",
    "# Compute trip counts\n",
    "same_borough_trips = df.filter(col(\"pickup_borough\") == col(\"dropoff_borough\")).groupBy(\"pickup_borough\").agg(count(\"*\").alias(\"same_borough_trips\"))\n",
    "diff_borough_trips = df.filter(col(\"pickup_borough\") != col(\"dropoff_borough\")).groupBy(\"pickup_borough\", \"dropoff_borough\").agg(count(\"*\").alias(\"diff_borough_trips\"))\n",
    "\n",
    "# Show results – for testing purposes\n",
    "# taxi_utilization.show()\n",
    "# taxi_next_fare.show()\n",
    "# same_borough_trips.show()\n",
    "# diff_borough_trips.show()\n",
    "# total_zero_idle_time = df.filter(col(\"idle_time\") == 0).count()\n",
    "# print(total_zero_idle_time)\n",
    "\n",
    "taxi_next_fare_renamed = taxi_next_fare.withColumnRenamed(\"dropoff_borough\", \"dropoff_borough_nf\")\n",
    "\n",
    "df_final = df.select(\"medallion\", \"pickup_borough\", \"dropoff_borough\", \"trip_duration\", \"idle_time\") \\\n",
    "             .join(taxi_utilization, \"medallion\", \"left\") \\\n",
    "             .join(taxi_next_fare_renamed, df[\"dropoff_borough\"] == taxi_next_fare_renamed[\"dropoff_borough_nf\"], \"left\") \\\n",
    "             .join(same_borough_trips, \"pickup_borough\", \"left\") \\\n",
    "             .join(diff_borough_trips, [\"pickup_borough\", \"dropoff_borough\"], \"left\") \\\n",
    "             .drop(\"dropoff_borough_nf\")\n",
    "\n",
    "df_final.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/full_results_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irrpT1A1g7aK",
    "outputId": "462a8f1f-214f-44f7-9cfa-97e5985f0193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+---------+-------------+---------+-----------+---------------------+------------------+------------------+\n",
      "|pickup_borough|dropoff_borough|medallion|trip_duration|idle_time|utilization|avg_time_to_next_fare|same_borough_trips|diff_borough_trips|\n",
      "+--------------+---------------+---------+-------------+---------+-----------+---------------------+------------------+------------------+\n",
      "+--------------+---------------+---------+-------------+---------+-----------+---------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"ParquetFileReader\").getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"../data/full_results_parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST END!@!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
