{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPNGXbbKPMRJ"
   },
   "source": [
    "# Environment prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sygNLpVOERC",
    "outputId": "07f36f2d-a2b2-46c8-f81d-e1423902bc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/arthas/anaconda3/lib/python3.10/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/arthas/anaconda3/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfiETFz0Or6J",
    "outputId": "d281f28f-d05d-45bb-9b0b-b1b5e3639601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /Users/arthas/anaconda3/lib/python3.10/site-packages (2.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8mBpIcxlO3dQ",
    "outputId": "1e78e8d9-8435-4cd5-f799-254d97bc348a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/30 21:37:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, udf, count, rank, lit, when, struct, collect_list, median, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDM_project_2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "85nGNasKPCYo"
   },
   "outputs": [],
   "source": [
    "# Define schema for taxi data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8Ub_lKBPJPS"
   },
   "source": [
    "# Data cleaning (Query 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-n0Dco0UPHTc",
    "outputId": "571940cc-aadf-4b6e-ea00-b18baef8ec46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0 - Sample of Cleaned Data:\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|FF0622D5D4D01EB3C...|E1E2DD8B97AB23CC6...|2013-11-15 21:39:56|2013-11-15 21:45:19|              322|          1.1|        -73.9776|      40.786705|       -73.981133|       40.774364|         CRD|        6.0|      0.5|    0.5|       1.4|         0.0|\n",
      "|9971C99B04C4D75DC...|9719D86AB73CC9F37...|2013-11-15 21:24:37|2013-11-15 21:45:21|             1243|          3.3|      -73.953903|      40.774883|       -73.988106|        40.74369|         CRD|       15.5|      0.5|    0.5|       3.3|         0.0|\n",
      "|506DF1356EA4AAA9D...|8615458D224BDD107...|2013-11-15 21:27:19|2013-11-15 21:45:26|             1087|          5.1|         -73.959|      40.768002|       -73.959541|       40.714104|         CSH|       18.5|      0.5|    0.5|       0.0|         0.0|\n",
      "|11AD539AEC155D686...|BC01267D1B02FA153...|2013-11-15 21:38:04|2013-11-15 21:45:34|              449|          1.2|      -73.945381|      40.711464|       -73.964523|       40.714104|         CSH|        7.0|      0.5|    0.5|       0.0|         0.0|\n",
      "|BCDD2F6283E47BDE3...|699CD80A25DA097EF...|2013-11-15 21:41:20|2013-11-15 21:45:40|              259|          0.9|      -73.944679|      40.779446|       -73.956261|       40.775719|         CRD|        5.5|      0.5|    0.5|       1.0|         0.0|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./sorted_data_1gb\"\n",
    "df = spark.read.csv(data_path, schema = schema, header = True)\n",
    "\n",
    "cleaned_df = df.filter(\n",
    "    (col(\"pickup_longitude\").isNotNull()) &\n",
    "    (col(\"pickup_latitude\").isNotNull()) &\n",
    "    (col(\"dropoff_longitude\").isNotNull()) &\n",
    "    (col(\"dropoff_latitude\").isNotNull()) &\n",
    "    (col(\"pickup_longitude\") != 0) &\n",
    "    (col(\"pickup_latitude\") != 0) &\n",
    "    (col(\"dropoff_longitude\") != 0) &\n",
    "    (col(\"dropoff_latitude\") != 0) &\n",
    "    (col(\"medallion\").isNotNull()) &\n",
    "    (col(\"hack_license\").isNotNull()) &\n",
    "    (col(\"trip_time_in_secs\") > 0) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"fare_amount\") > 0)\n",
    ")\n",
    "\n",
    "print(\"Query 0 - Sample of Cleaned Data:\")\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XEFlFuPYHaK",
    "outputId": "133787ed-5a8d-4783-df9c-3ca8823d55c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Cell Sample:\n",
      "+---------------+----------------+---------------+----------------+\n",
      "|pickup_grid_500|dropoff_grid_500|pickup_grid_250|dropoff_grid_250|\n",
      "+---------------+----------------+---------------+----------------+\n",
      "|       -152.156|        -155.156|       -305.312|        -311.311|\n",
      "|       -155.160|        -162.154|       -310.320|        -324.308|\n",
      "|       -156.159|        -168.159|       -313.318|        -337.318|\n",
      "|       -169.162|        -168.158|       -338.323|        -337.316|\n",
      "|       -154.162|        -155.160|       -308.323|        -310.319|\n",
      "+---------------+----------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_grid_cell(lat, lon, cell_size_m):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    origin_lat, origin_lon = 41.474937, -74.913585\n",
    "    lat_per_cell = cell_size_m / 111000.0\n",
    "    lon_per_cell = cell_size_m / (111000.0 * math.cos(math.radians(origin_lat)))\n",
    "    lat_offset = math.floor((lat - origin_lat) / lat_per_cell) + 1\n",
    "    lon_offset = math.floor((lon - origin_lon) / lon_per_cell) + 1\n",
    "    return f\"{lat_offset}.{lon_offset}\"\n",
    "\n",
    "grid_cell_500_udf = udf(lambda lat, lon: get_grid_cell(lat, lon, 500), StringType())\n",
    "grid_cell_250_udf = udf(lambda lat, lon: get_grid_cell(lat, lon, 250), StringType())\n",
    "\n",
    "# Grid transformation\n",
    "df_grid = cleaned_df \\\n",
    "    .withColumn(\"pickup_grid_500\", grid_cell_500_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_500\", grid_cell_500_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .withColumn(\"pickup_grid_250\", grid_cell_250_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_250\", grid_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .filter(col(\"pickup_grid_500\").isNotNull() & col(\"dropoff_grid_500\").isNotNull() &\n",
    "            col(\"pickup_grid_250\").isNotNull() & col(\"dropoff_grid_250\").isNotNull())\n",
    "\n",
    "print(\"Grid Cell Sample:\")\n",
    "df_grid.select(\"pickup_grid_500\", \"dropoff_grid_500\", \"pickup_grid_250\", \"dropoff_grid_250\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMWpqamXRFuk"
   },
   "source": [
    "# Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZgxG1oul2Xf"
   },
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 21:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 21:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 21:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 20:======================================>                 (11 + 5) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|start_cell|end_cell|ride_count|\n",
      "+----------+--------+----------+\n",
      "|  -154.156|-152.156|         2|\n",
      "|  -164.155|-162.156|         2|\n",
      "|  -159.157|-162.152|         1|\n",
      "|  -167.154|-165.155|         1|\n",
      "|  -168.155|-157.154|         1|\n",
      "|  -160.176|-161.174|         1|\n",
      "|  -158.158|-160.152|         1|\n",
      "|  -160.154|-158.154|         1|\n",
      "|  -159.157|-162.155|         1|\n",
      "|  -158.159|-162.168|         1|\n",
      "+----------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 21:57:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 21:57:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 21:57:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 21:57:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a fixed reference timestamp\n",
    "reference_time = df_grid.agg(F.max(\"pickup_datetime\")).collect()[0][0]\n",
    "\n",
    "# Filter the data for trips that occurred in the last 30 minutes relative to the reference time\n",
    "time_limit = F.lit(reference_time) - F.expr(\"INTERVAL 30 MINUTES\")\n",
    "\n",
    "# Filter the DataFrame to include only trips completed in the last 30 minutes from the reference time\n",
    "df_recent = df_grid.filter(F.col(\"pickup_datetime\") >= time_limit)\n",
    "\n",
    "# Perform the groupBy aggregation to find frequent routes within the last 30 minutes\n",
    "route_counts = df_recent \\\n",
    "    .groupBy(\"pickup_grid_500\", \"dropoff_grid_500\") \\\n",
    "    .agg(F.count(\"*\").alias(\"ride_count\"))\n",
    "\n",
    "# Rank routes by their frequency\n",
    "window_spec = Window.orderBy(F.col(\"ride_count\").desc())\n",
    "\n",
    "# Apply the window function to rank the routes\n",
    "top_10_routes_static = route_counts \\\n",
    "    .withColumn(\"rank\", F.rank().over(window_spec)) \\\n",
    "    .filter(F.col(\"rank\") <= 10)\n",
    "\n",
    "# Select only the relevant columns for output: start cell, end cell, and number of rides\n",
    "top_10_routes_static = top_10_routes_static.select(\n",
    "    F.col(\"pickup_grid_500\").alias(\"start_cell\"),\n",
    "    F.col(\"dropoff_grid_500\").alias(\"end_cell\"),\n",
    "    \"ride_count\"\n",
    ")\n",
    "\n",
    "# Show the top 10 frequent routes (for verification)\n",
    "top_10_routes_static.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===============>                                         (3 + 8) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------------+----------+----+----------------+\n",
      "|              window|pickup_grid_500|dropoff_grid_500|ride_count|rank|processing_delay|\n",
      "+--------------------+---------------+----------------+----------+----+----------------+\n",
      "|{2013-01-01 04:00...|       -162.154|        -167.153|         2|   1|       386358070|\n",
      "|{2013-01-01 04:00...|       -163.155|        -169.151|         2|   1|       386358070|\n",
      "|{2013-01-01 04:00...|       -158.154|        -155.160|         2|   1|       386358070|\n",
      "|{2013-01-01 04:00...|       -150.161|        -149.161|         1|   4|       386358070|\n",
      "|{2013-01-01 04:00...|       -152.162|        -152.160|         1|   4|       386358070|\n",
      "|{2013-01-01 04:00...|       -153.156|        -159.154|         1|   4|       386358070|\n",
      "|{2013-01-01 04:00...|       -155.161|        -154.162|         1|   4|       386358070|\n",
      "|{2013-01-01 04:00...|       -156.153|        -167.153|         1|   4|       386358070|\n",
      "|{2013-01-01 04:00...|       -156.155|        -148.161|         1|   4|       386358070|\n",
      "|{2013-01-01 04:00...|       -156.159|        -157.158|         1|   4|       386358070|\n",
      "+--------------------+---------------+----------------+----------+----+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "windowed_routes = df_grid \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "    .groupBy(window(\"dropoff_datetime\", \"30 minutes\"), \"pickup_grid_500\", \"dropoff_grid_500\") \\\n",
    "    .agg(count(\"*\").alias(\"ride_count\"))\n",
    "\n",
    "# Rank and filter top 10\n",
    "from pyspark.sql import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"window\").orderBy(col(\"ride_count\").desc())\n",
    "\n",
    "top_routes = windowed_routes \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 10)\n",
    "\n",
    "# Add processing delay\n",
    "top_routes = top_routes.withColumn(\"processing_delay\", (current_timestamp() - col(\"window.start\")).cast(\"long\"))\n",
    "\n",
    "top_routes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_datetime` cannot be resolved. Did you mean one of the following? [`pickup_grid_500`, `ride_count`, `dropoff_grid_500`].;\n'Filter (('pickup_datetime >= cast(current_timestamp() - INTERVAL '30' MINUTE as timestamp)) AND ('pickup_datetime <= current_timestamp()))\n+- Aggregate [pickup_grid_500#115, dropoff_grid_500#134], [pickup_grid_500#115, dropoff_grid_500#134, count(1) AS ride_count#296L]\n   +- Filter (pickup_datetime#2 >= cast(2013-12-31 23:59:55 - INTERVAL '30' MINUTE as timestamp))\n      +- Filter (((isnotnull(pickup_grid_500#115) AND isnotnull(dropoff_grid_500#134)) AND isnotnull(pickup_grid_250#154)) AND isnotnull(dropoff_grid_250#175))\n         +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_grid_500#115, dropoff_grid_500#134, pickup_grid_250#154, <lambda>(dropoff_latitude#9, dropoff_longitude#8)#174 AS dropoff_grid_250#175]\n            +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_grid_500#115, dropoff_grid_500#134, <lambda>(pickup_latitude#7, pickup_longitude#6)#153 AS pickup_grid_250#154]\n               +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_grid_500#115, <lambda>(dropoff_latitude#9, dropoff_longitude#8)#133 AS dropoff_grid_500#134]\n                  +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, <lambda>(pickup_latitude#7, pickup_longitude#6)#114 AS pickup_grid_500#115]\n                     +- Filter ((((((((((((isnotnull(pickup_longitude#6) AND isnotnull(pickup_latitude#7)) AND isnotnull(dropoff_longitude#8)) AND isnotnull(dropoff_latitude#9)) AND NOT (pickup_longitude#6 = cast(0 as double))) AND NOT (pickup_latitude#7 = cast(0 as double))) AND NOT (dropoff_longitude#8 = cast(0 as double))) AND NOT (dropoff_latitude#9 = cast(0 as double))) AND isnotnull(medallion#0)) AND isnotnull(hack_license#1)) AND (trip_time_in_secs#4 > 0)) AND (trip_distance#5 > cast(0 as double))) AND (fare_amount#11 > cast(0 as double)))\n                        +- Relation [medallion#0,hack_license#1,pickup_datetime#2,dropoff_datetime#3,trip_time_in_secs#4,trip_distance#5,pickup_longitude#6,pickup_latitude#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,surcharge#12,mta_tax#13,tip_amount#14,tolls_amount#15] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m current_time \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcurrent_timestamp()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Filter the data for the last 30 minutes\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m \u001b[43mroute_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mINTERVAL 30 MINUTE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcurrent_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Select relevant columns and compute the rank based on ride_count\u001b[39;00m\n\u001b[1;32m     19\u001b[0m top_10_routes \u001b[38;5;241m=\u001b[39m filtered_data\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m )\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mrank()\u001b[38;5;241m.\u001b[39mover(window_spec)) \\\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3331\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3329\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3330\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3331\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3334\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3335\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3336\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_datetime` cannot be resolved. Did you mean one of the following? [`pickup_grid_500`, `ride_count`, `dropoff_grid_500`].;\n'Filter (('pickup_datetime >= cast(current_timestamp() - INTERVAL '30' MINUTE as timestamp)) AND ('pickup_datetime <= current_timestamp()))\n+- Aggregate [pickup_grid_500#115, dropoff_grid_500#134], [pickup_grid_500#115, dropoff_grid_500#134, count(1) AS ride_count#296L]\n   +- Filter (pickup_datetime#2 >= cast(2013-12-31 23:59:55 - INTERVAL '30' MINUTE as timestamp))\n      +- Filter (((isnotnull(pickup_grid_500#115) AND isnotnull(dropoff_grid_500#134)) AND isnotnull(pickup_grid_250#154)) AND isnotnull(dropoff_grid_250#175))\n         +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_grid_500#115, dropoff_grid_500#134, pickup_grid_250#154, <lambda>(dropoff_latitude#9, dropoff_longitude#8)#174 AS dropoff_grid_250#175]\n            +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_grid_500#115, dropoff_grid_500#134, <lambda>(pickup_latitude#7, pickup_longitude#6)#153 AS pickup_grid_250#154]\n               +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, pickup_grid_500#115, <lambda>(dropoff_latitude#9, dropoff_longitude#8)#133 AS dropoff_grid_500#134]\n                  +- Project [medallion#0, hack_license#1, pickup_datetime#2, dropoff_datetime#3, trip_time_in_secs#4, trip_distance#5, pickup_longitude#6, pickup_latitude#7, dropoff_longitude#8, dropoff_latitude#9, payment_type#10, fare_amount#11, surcharge#12, mta_tax#13, tip_amount#14, tolls_amount#15, <lambda>(pickup_latitude#7, pickup_longitude#6)#114 AS pickup_grid_500#115]\n                     +- Filter ((((((((((((isnotnull(pickup_longitude#6) AND isnotnull(pickup_latitude#7)) AND isnotnull(dropoff_longitude#8)) AND isnotnull(dropoff_latitude#9)) AND NOT (pickup_longitude#6 = cast(0 as double))) AND NOT (pickup_latitude#7 = cast(0 as double))) AND NOT (dropoff_longitude#8 = cast(0 as double))) AND NOT (dropoff_latitude#9 = cast(0 as double))) AND isnotnull(medallion#0)) AND isnotnull(hack_license#1)) AND (trip_time_in_secs#4 > 0)) AND (trip_distance#5 > cast(0 as double))) AND (fare_amount#11 > cast(0 as double)))\n                        +- Relation [medallion#0,hack_license#1,pickup_datetime#2,dropoff_datetime#3,trip_time_in_secs#4,trip_distance#5,pickup_longitude#6,pickup_latitude#7,dropoff_longitude#8,dropoff_latitude#9,payment_type#10,fare_amount#11,surcharge#12,mta_tax#13,tip_amount#14,tolls_amount#15] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assuming route_counts is the static dataset that contains necessary columns (pickup_datetime, dropoff_datetime, etc.)\n",
    "\n",
    "# Define window specification for ranking by ride_count\n",
    "window_spec = Window.orderBy(F.col(\"ride_count\").desc())\n",
    "\n",
    "# Define the current timestamp\n",
    "current_time = F.current_timestamp()\n",
    "\n",
    "# Filter the data for the last 30 minutes\n",
    "filtered_data = route_counts.filter(\n",
    "    (F.col(\"pickup_datetime\") >= (current_time - F.expr(\"INTERVAL 30 MINUTE\"))) &\n",
    "    (F.col(\"pickup_datetime\") <= current_time)\n",
    ")\n",
    "\n",
    "# Select relevant columns and compute the rank based on ride_count\n",
    "top_10_routes = filtered_data.select(\n",
    "    \"pickup_datetime\", \n",
    "    \"dropoff_datetime\", \n",
    "    \"pickup_grid_500\", \n",
    "    \"dropoff_grid_500\", \n",
    "    \"ride_count\"\n",
    ").withColumn(\"rank\", F.rank().over(window_spec)) \\\n",
    "    .filter(F.col(\"rank\") <= 10)\n",
    "\n",
    "# Add processing delay (current time - pickup time)\n",
    "top_10_routes = top_10_routes.withColumn(\"processing_delay\", \n",
    "    (current_time - F.col(\"pickup_datetime\")).cast(\"long\"))\n",
    "\n",
    "# Add NULL values for missing routes if fewer than 10 routes are found\n",
    "top_10_routes_with_nulls = top_10_routes.select(\n",
    "    \"pickup_datetime\", \n",
    "    \"dropoff_datetime\", \n",
    "    F.coalesce(F.col(\"pickup_grid_500\"), F.lit(None)).alias(\"start_cell_id\"),\n",
    "    F.coalesce(F.col(\"dropoff_grid_500\"), F.lit(None)).alias(\"end_cell_id\"),\n",
    "    F.coalesce(F.col(\"processing_delay\"), F.lit(None)).alias(\"delay\")\n",
    ")\n",
    "\n",
    "# Handle the case of fewer than 10 routes by adding NULL rows for missing data\n",
    "null_routes = spark.createDataFrame(\n",
    "    [(None, None, None, None, None) for _ in range(10 - top_10_routes_with_nulls.count())],\n",
    "    [\"pickup_datetime\", \"dropoff_datetime\", \"start_cell_id\", \"end_cell_id\", \"delay\"]\n",
    ")\n",
    "\n",
    "# Union the results with the NULL rows to ensure 10 rows\n",
    "top_10_routes_with_nulls = top_10_routes_with_nulls.unionByName(null_routes)\n",
    "\n",
    "# To make sure only the last 10 rows are returned (in case there are more than 10)\n",
    "top_10_routes_with_nulls = top_10_routes_with_nulls.limit(10)\n",
    "\n",
    "# Add the delay capture logic (this will be the time difference between reading input and producing output)\n",
    "top_10_routes_with_nulls = top_10_routes_with_nulls.withColumn(\"output_produced_time\", current_time)\n",
    "\n",
    "# Show the result (or write to a file/database)\n",
    "top_10_routes_with_nulls.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJPwCt4AWZXW"
   },
   "source": [
    "# Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkHtv40El4wP"
   },
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k81v4PM6f4cs",
    "outputId": "f8ac875c-4ed3-46e8-f414-a32d33cce2db"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, median, count, when, rank\n",
    "\n",
    "# Add profit column and 250m grid cells\n",
    "profit_df = cleaned_df \\\n",
    "    .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "    .withColumn(\"pickup_grid_250\", grid_cell_250_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_250\", grid_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .filter(col(\"pickup_grid_250\").isNotNull() & col(\"dropoff_grid_250\").isNotNull())\n",
    "\n",
    "# Step 1: Calculate median profit per pickup cell\n",
    "median_profit = profit_df \\\n",
    "    .groupBy(\"pickup_grid_250\") \\\n",
    "    .agg(median(\"profit\").alias(\"median_profit\"))\n",
    "\n",
    "# Step 2: Estimate empty taxis (simplified for static data)\n",
    "# For each medallion, count dropoffs; assume a taxi is \"empty\" if it has no subsequent pickup within 30 minutes\n",
    "# In static data, we'll approximate by counting unique dropoffs per cell without time tracking\n",
    "empty_taxis = profit_df \\\n",
    "    .groupBy(\"medallion\", \"dropoff_grid_250\", \"dropoff_datetime\") \\\n",
    "    .agg(count(\"*\").alias(\"trip_count\")) \\\n",
    "    .groupBy(\"dropoff_grid_250\") \\\n",
    "    .agg(count(\"medallion\").alias(\"empty_taxis\"))  # Simplified: counts unique medallions per dropoff cell\n",
    "\n",
    "# Step 3: Compute profitability\n",
    "profitability = median_profit \\\n",
    "    .join(empty_taxis, median_profit.pickup_grid_250 == empty_taxis.dropoff_grid_250, \"left_outer\") \\\n",
    "    .na.fill({\"empty_taxis\": 0}) \\\n",
    "    .withColumn(\"profitability\",\n",
    "                when(col(\"empty_taxis\") > 0, col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "                .otherwise(col(\"median_profit\"))) \\\n",
    "    .select(col(\"pickup_grid_250\").alias(\"cell_id\"), \"empty_taxis\", \"median_profit\", \"profitability\")\n",
    "\n",
    "# Step 4: Rank and get top 10\n",
    "window_spec_profit = Window.orderBy(col(\"profitability\").desc())\n",
    "top_10_profit_static = profitability \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec_profit)) \\\n",
    "    .filter(col(\"rank\") <= 10) \\\n",
    "    .select(\"cell_id\", \"empty_taxis\", \"median_profit\", \"profitability\", \"rank\")\n",
    "\n",
    "# Show results\n",
    "print(\"Query 2 Part 1 - Top 10 Profitable Areas (Static):\")\n",
    "top_10_profit_static.show(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
