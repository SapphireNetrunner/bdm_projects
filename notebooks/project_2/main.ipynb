{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPNGXbbKPMRJ"
   },
   "source": [
    "# Environment prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sygNLpVOERC",
    "outputId": "07f36f2d-a2b2-46c8-f81d-e1423902bc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/arthas/anaconda3/lib/python3.10/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/arthas/anaconda3/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfiETFz0Or6J",
    "outputId": "d281f28f-d05d-45bb-9b0b-b1b5e3639601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /Users/arthas/anaconda3/lib/python3.10/site-packages (2.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8mBpIcxlO3dQ",
    "outputId": "1e78e8d9-8435-4cd5-f799-254d97bc348a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/30 23:08:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, udf, count, rank, lit, when, struct, collect_list, median, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDM_project_2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "85nGNasKPCYo"
   },
   "outputs": [],
   "source": [
    "# Define schema for taxi data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8Ub_lKBPJPS"
   },
   "source": [
    "# Data cleaning (Query 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-n0Dco0UPHTc",
    "outputId": "571940cc-aadf-4b6e-ea00-b18baef8ec46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0 - Sample of Cleaned Data:\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|FF0622D5D4D01EB3C...|E1E2DD8B97AB23CC6...|2013-11-15 21:39:56|2013-11-15 21:45:19|              322|          1.1|        -73.9776|      40.786705|       -73.981133|       40.774364|         CRD|        6.0|      0.5|    0.5|       1.4|         0.0|\n",
      "|9971C99B04C4D75DC...|9719D86AB73CC9F37...|2013-11-15 21:24:37|2013-11-15 21:45:21|             1243|          3.3|      -73.953903|      40.774883|       -73.988106|        40.74369|         CRD|       15.5|      0.5|    0.5|       3.3|         0.0|\n",
      "|506DF1356EA4AAA9D...|8615458D224BDD107...|2013-11-15 21:27:19|2013-11-15 21:45:26|             1087|          5.1|         -73.959|      40.768002|       -73.959541|       40.714104|         CSH|       18.5|      0.5|    0.5|       0.0|         0.0|\n",
      "|11AD539AEC155D686...|BC01267D1B02FA153...|2013-11-15 21:38:04|2013-11-15 21:45:34|              449|          1.2|      -73.945381|      40.711464|       -73.964523|       40.714104|         CSH|        7.0|      0.5|    0.5|       0.0|         0.0|\n",
      "|BCDD2F6283E47BDE3...|699CD80A25DA097EF...|2013-11-15 21:41:20|2013-11-15 21:45:40|              259|          0.9|      -73.944679|      40.779446|       -73.956261|       40.775719|         CRD|        5.5|      0.5|    0.5|       1.0|         0.0|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./sorted_data_1gb\"\n",
    "df = spark.read.csv(data_path, schema = schema, header = True)\n",
    "\n",
    "cleaned_df = df.filter(\n",
    "    (col(\"pickup_longitude\").isNotNull()) &\n",
    "    (col(\"pickup_latitude\").isNotNull()) &\n",
    "    (col(\"dropoff_longitude\").isNotNull()) &\n",
    "    (col(\"dropoff_latitude\").isNotNull()) &\n",
    "    (col(\"pickup_longitude\") != 0) &\n",
    "    (col(\"pickup_latitude\") != 0) &\n",
    "    (col(\"dropoff_longitude\") != 0) &\n",
    "    (col(\"dropoff_latitude\") != 0) &\n",
    "    (col(\"medallion\").isNotNull()) &\n",
    "    (col(\"hack_license\").isNotNull()) &\n",
    "    (col(\"trip_time_in_secs\") > 0) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"fare_amount\") > 0)\n",
    ")\n",
    "\n",
    "print(\"Query 0 - Sample of Cleaned Data:\")\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XEFlFuPYHaK",
    "outputId": "133787ed-5a8d-4783-df9c-3ca8823d55c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Cell Sample:\n",
      "+---------------+----------------+---------------+----------------+\n",
      "|pickup_grid_500|dropoff_grid_500|pickup_grid_250|dropoff_grid_250|\n",
      "+---------------+----------------+---------------+----------------+\n",
      "|       -152.156|        -155.156|       -305.312|        -311.311|\n",
      "|       -155.160|        -162.154|       -310.320|        -324.308|\n",
      "|       -156.159|        -168.159|       -313.318|        -337.318|\n",
      "|       -169.162|        -168.158|       -338.323|        -337.316|\n",
      "|       -154.162|        -155.160|       -308.323|        -310.319|\n",
      "+---------------+----------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_grid_cell(lat, lon, cell_size_m):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    origin_lat, origin_lon = 41.474937, -74.913585\n",
    "    lat_per_cell = cell_size_m / 111000.0\n",
    "    lon_per_cell = cell_size_m / (111000.0 * math.cos(math.radians(origin_lat)))\n",
    "    lat_offset = math.floor((lat - origin_lat) / lat_per_cell) + 1\n",
    "    lon_offset = math.floor((lon - origin_lon) / lon_per_cell) + 1\n",
    "    return f\"{lat_offset}.{lon_offset}\"\n",
    "\n",
    "grid_cell_500_udf = udf(lambda lat, lon: get_grid_cell(lat, lon, 500), StringType())\n",
    "grid_cell_250_udf = udf(lambda lat, lon: get_grid_cell(lat, lon, 250), StringType())\n",
    "\n",
    "# Grid transformation\n",
    "df_grid = cleaned_df \\\n",
    "    .withColumn(\"pickup_grid_500\", grid_cell_500_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_500\", grid_cell_500_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .withColumn(\"pickup_grid_250\", grid_cell_250_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_250\", grid_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .filter(col(\"pickup_grid_500\").isNotNull() & col(\"dropoff_grid_500\").isNotNull() &\n",
    "            col(\"pickup_grid_250\").isNotNull() & col(\"dropoff_grid_250\").isNotNull())\n",
    "\n",
    "print(\"Grid Cell Sample:\")\n",
    "df_grid.select(\"pickup_grid_500\", \"dropoff_grid_500\", \"pickup_grid_250\", \"dropoff_grid_250\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMWpqamXRFuk"
   },
   "source": [
    "# Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZgxG1oul2Xf"
   },
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 23:09:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 5:=======================================>                 (11 + 5) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|start_cell|end_cell|ride_count|\n",
      "+----------+--------+----------+\n",
      "|  -167.154|-165.155|         1|\n",
      "|  -168.155|-157.154|         1|\n",
      "|  -160.176|-161.174|         1|\n",
      "|  -158.158|-160.152|         1|\n",
      "|  -160.154|-158.154|         1|\n",
      "|  -154.161|-151.163|         1|\n",
      "|  -167.153|-179.155|         1|\n",
      "|  -162.152|-117.178|         1|\n",
      "|  -154.161|-155.159|         1|\n",
      "|  -154.156|-160.157|         1|\n",
      "+----------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 23:09:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a fixed reference timestamp\n",
    "reference_time = df_grid.agg(F.max(\"dropoff_datetime\")).collect()[0][0]\n",
    "\n",
    "# Filter the data for trips that occurred in the last 30 minutes relative to the reference time\n",
    "time_limit = F.lit(reference_time) - F.expr(\"INTERVAL 30 MINUTES\")\n",
    "\n",
    "# Filter the DataFrame to include only trips completed in the last 30 minutes from the reference time\n",
    "df_recent = df_grid.filter(F.col(\"dropoff_datetime\") >= time_limit)\n",
    "\n",
    "# Perform the groupBy aggregation to find frequent routes within the last 30 minutes\n",
    "route_counts = df_recent \\\n",
    "    .groupBy(\"pickup_grid_500\", \"dropoff_grid_500\") \\\n",
    "    .agg(F.count(\"*\").alias(\"ride_count\"))\n",
    "\n",
    "# Rank routes by their frequency\n",
    "window_spec = Window.orderBy(F.col(\"ride_count\").desc())\n",
    "\n",
    "# Apply the window function to rank the routes\n",
    "top_10_routes_static = route_counts \\\n",
    "    .withColumn(\"rank\", F.rank().over(window_spec)) \\\n",
    "    .filter(F.col(\"rank\") <= 10)\n",
    "\n",
    "# Select only the relevant columns for output: start cell, end cell, and number of rides\n",
    "top_10_routes_static = top_10_routes_static.select(\n",
    "    F.col(\"pickup_grid_500\").alias(\"start_cell\"),\n",
    "    F.col(\"dropoff_grid_500\").alias(\"end_cell\"),\n",
    "    \"ride_count\"\n",
    ")\n",
    "\n",
    "# Show the top 10 frequent routes\n",
    "top_10_routes_static.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===============>                                         (3 + 8) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+--------+---------+\n",
      "|    pickup_datetime|   dropoff_datetime|start_cell|end_cell|    delay|\n",
      "+-------------------+-------------------+----------+--------+---------+\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -162.154|-167.153|386363342|\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -163.155|-169.151|386363342|\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -158.154|-155.160|386363342|\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -150.161|-149.161|386363342|\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -152.162|-152.160|386363342|\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -153.156|-159.154|386363342|\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -155.161|-154.162|386363342|\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -156.153|-167.153|386363342|\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -156.155|-148.161|386363342|\n",
      "|2013-01-01 04:00:00|2013-01-01 04:30:00|  -156.159|-157.158|386363342|\n",
      "+-------------------+-------------------+----------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_grid = df_grid.withColumn(\"event_time\", col(\"dropoff_datetime\"))\n",
    "\n",
    "# Apply watermarking to track events dynamically\n",
    "windowed_routes = df_grid \\\n",
    "    .withWatermark(\"event_time\", \"30 minutes\") \\\n",
    "    .groupBy(window(\"event_time\", \"30 minutes\"), \"pickup_grid_500\", \"dropoff_grid_500\") \\\n",
    "    .agg(count(\"*\").alias(\"ride_count\"))\n",
    "\n",
    "# Rank routes dynamically within each 30-minute window\n",
    "window_spec = Window.partitionBy(\"window\").orderBy(col(\"ride_count\").desc())\n",
    "\n",
    "top_routes = windowed_routes \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 10)\n",
    "\n",
    "# Processing delay\n",
    "top_routes = top_routes.withColumn(\"delay\", (current_timestamp() - col(\"window.start\")).cast(\"long\"))\n",
    "\n",
    "final_result = top_routes.select(\n",
    "    col(\"window.start\").alias(\"pickup_datetime\"),\n",
    "    col(\"window.end\").alias(\"dropoff_datetime\"),\n",
    "    col(\"pickup_grid_500\").alias(\"start_cell\"),\n",
    "    col(\"dropoff_grid_500\").alias(\"end_cell\"),\n",
    "    col(\"delay\")\n",
    ")\n",
    "\n",
    "final_result.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJPwCt4AWZXW"
   },
   "source": [
    "# Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkHtv40El4wP"
   },
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k81v4PM6f4cs",
    "outputId": "f8ac875c-4ed3-46e8-f414-a32d33cce2db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 23:09:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 Part 1 - Top 10 Profitable Areas (Static):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 23:09:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/30 23:09:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+-------------+----+\n",
      "| cell_id|empty_taxis|median_profit|profitability|rank|\n",
      "+--------+-----------+-------------+-------------+----+\n",
      "|-128.590|          0|       504.77|       504.77|   1|\n",
      "| -504.95|          0|        360.0|        360.0|   2|\n",
      "|-386.228|          0|        300.0|        300.0|   3|\n",
      "|-256.372|          0|        296.4|        296.4|   4|\n",
      "|-173.480|          1|        288.0|        288.0|   5|\n",
      "|-365.113|          1|        288.0|        288.0|   5|\n",
      "| -387.75|          0|        273.0|        273.0|   7|\n",
      "|-157.512|          0|        270.0|        270.0|   8|\n",
      "|-332.283|          1|        260.0|        260.0|   9|\n",
      "|-146.495|          0|        250.0|        250.0|  10|\n",
      "+--------+-----------+-------------+-------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 23:09:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, median, count, when, rank\n",
    "\n",
    "# Add profit column and 250m grid cells\n",
    "profit_df = cleaned_df \\\n",
    "    .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "    .withColumn(\"pickup_grid_250\", grid_cell_250_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_250\", grid_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .filter(col(\"pickup_grid_250\").isNotNull() & col(\"dropoff_grid_250\").isNotNull())\n",
    "\n",
    "# Step 1: Calculate median profit per pickup cell\n",
    "median_profit = profit_df \\\n",
    "    .groupBy(\"pickup_grid_250\") \\\n",
    "    .agg(median(\"profit\").alias(\"median_profit\"))\n",
    "\n",
    "# Step 2: Estimate empty taxis (simplified for static data)\n",
    "# For each medallion, count dropoffs; assume a taxi is \"empty\" if it has no subsequent pickup within 30 minutes\n",
    "# In static data, we'll approximate by counting unique dropoffs per cell without time tracking\n",
    "empty_taxis = profit_df \\\n",
    "    .groupBy(\"medallion\", \"dropoff_grid_250\", \"dropoff_datetime\") \\\n",
    "    .agg(count(\"*\").alias(\"trip_count\")) \\\n",
    "    .groupBy(\"dropoff_grid_250\") \\\n",
    "    .agg(count(\"medallion\").alias(\"empty_taxis\"))  # Simplified: counts unique medallions per dropoff cell\n",
    "\n",
    "# Step 3: Compute profitability\n",
    "profitability = median_profit \\\n",
    "    .join(empty_taxis, median_profit.pickup_grid_250 == empty_taxis.dropoff_grid_250, \"left_outer\") \\\n",
    "    .na.fill({\"empty_taxis\": 0}) \\\n",
    "    .withColumn(\"profitability\",\n",
    "                when(col(\"empty_taxis\") > 0, col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "                .otherwise(col(\"median_profit\"))) \\\n",
    "    .select(col(\"pickup_grid_250\").alias(\"cell_id\"), \"empty_taxis\", \"median_profit\", \"profitability\")\n",
    "\n",
    "# Step 4: Rank and get top 10\n",
    "window_spec_profit = Window.orderBy(col(\"profitability\").desc())\n",
    "top_10_profit_static = profitability \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec_profit)) \\\n",
    "    .filter(col(\"rank\") <= 10) \\\n",
    "    .select(\"cell_id\", \"empty_taxis\", \"median_profit\", \"profitability\", \"rank\")\n",
    "\n",
    "# Show results\n",
    "print(\"Query 2 Part 1 - Top 10 Profitable Areas (Static):\")\n",
    "top_10_profit_static.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Q2P1 with time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 23:09:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 Part 1 - Top 10 Profitable Areas (Static):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 23:09:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/30 23:09:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 33:=================>                                      (5 + 10) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+-------------+----+\n",
      "|cell_id|empty_taxis|median_profit|profitability|rank|\n",
      "+-------+-----------+-------------+-------------+----+\n",
      "+-------+-----------+-------------+-------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, median, count, when, rank, max\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add profit column and 250m grid cells\n",
    "profit_df = cleaned_df \\\n",
    "    .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "    .withColumn(\"pickup_grid_250\", grid_cell_250_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_250\", grid_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .filter(col(\"pickup_grid_250\").isNotNull() & col(\"dropoff_grid_250\").isNotNull())\n",
    "\n",
    "# Step 1: Get the latest timestamp from the data\n",
    "latest_time = profit_df.agg(max(\"dropoff_datetime\")).collect()[0][0]\n",
    "\n",
    "# Step 2: Filter trips that started in the last 15 minutes relative to the latest timestamp\n",
    "profit_df_filtered = profit_df.filter(col(\"pickup_datetime\") >= (F.lit(latest_time) - F.expr(\"INTERVAL 15 MINUTES\")))\n",
    "\n",
    "# Step 3: Calculate the median profit per pickup grid cell (within the last 15 minutes)\n",
    "median_profit = profit_df_filtered \\\n",
    "    .groupBy(\"pickup_grid_250\") \\\n",
    "    .agg(F.expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\"))\n",
    "\n",
    "# Step 4: Estimate empty taxis (drop-offs that occurred in the last 30 minutes with no subsequent pickup)\n",
    "empty_taxis = profit_df \\\n",
    "    .filter(col(\"dropoff_datetime\") >= (F.lit(latest_time) - F.expr(\"INTERVAL 30 MINUTES\"))) \\\n",
    "    .withColumn(\"next_pickup\", F.lead(\"pickup_datetime\").over(Window.partitionBy(\"medallion\").orderBy(\"dropoff_datetime\"))) \\\n",
    "    .filter(F.col(\"next_pickup\").isNull() | (F.col(\"next_pickup\") > F.col(\"dropoff_datetime\"))) \\\n",
    "    .groupBy(\"dropoff_grid_250\") \\\n",
    "    .agg(F.countDistinct(\"medallion\").alias(\"empty_taxis\"))\n",
    "\n",
    "# Step 5: Compute profitability (median_profit / empty_taxis)\n",
    "profitability = median_profit \\\n",
    "    .join(empty_taxis, median_profit.pickup_grid_250 == empty_taxis.dropoff_grid_250, \"left_outer\") \\\n",
    "    .na.fill({\"empty_taxis\": 0}) \\\n",
    "    .withColumn(\"profitability\", \n",
    "                F.when(col(\"empty_taxis\") > 0, col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "                 .otherwise(col(\"median_profit\"))) \\\n",
    "    .select(col(\"pickup_grid_250\").alias(\"cell_id\"), \"empty_taxis\", \"median_profit\", \"profitability\")\n",
    "\n",
    "# Step 6: Rank by profitability and select the top 10 most profitable areas\n",
    "window_spec_profit = Window.orderBy(F.col(\"profitability\").desc())\n",
    "\n",
    "top_10_profit_static = profitability \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec_profit)) \\\n",
    "    .filter(F.col(\"rank\") <= 10) \\\n",
    "    .select(\"cell_id\", \"empty_taxis\", \"median_profit\", \"profitability\", \"rank\")\n",
    "\n",
    "print(\"Query 2 Part 1 - Top 10 Profitable Areas (Static):\")\n",
    "top_10_profit_static.show(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
